% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_building.R
\name{train_model}
\alias{train_model}
\title{Model training and validation}
\usage{
train_model(
  df = NULL,
  vec_label = NULL,
  hyper_choice = "defaults",
  train_ratio = 0.75
)
}
\arguments{
\item{df}{A data.frame or tibble contains features for classification.}

\item{vec_label}{A character vector that contains all behaviour type labels.}

\item{hyper_choice}{A character value. The default value is set to "defaults",
which will let the XGBoost model use a default hyperparameter set without further
hyperparameter tuning. With value "tune", the model will do hyperparameter tuning
with a predefined hyperparameter group. See details below.}

\item{train_ratio}{A double value determins the percentage of data used to train
the model, the remainder of the data being used for model validation.}
}
\value{
An XGBoost classifier for new data prediction.
}
\description{
Function for machine learning model training and validation. Usually, the training
and validation of supervised machine learning models includes three steps:
(1) machine learning model hyperparameter tuning by cross-validation; (2) model
training with the optimal hyperparameter set, and (3) evaluating model performance
with a test dataset. \code{train_model} is a wrapper function that utilizes relavant
functions from the "caret" package to automatically conduct the three steps for
model training and validation.
}
\details{
When the argument hyper_choice is set to "defaults", the arguments for
 XGBoost model are set as: nrounds = 10, max_depth = 6, eta = 0.3, gamma = 0,
 colsample_bytree = 1, min_child_weight = 1, subsample = 1. Value "tune" will
 let the function run hyperparameter tuning with the choices: nrounds = c(5, 10,
 50, 100), max_depth = c(2, 3, 4, 5, 6), eta = c(0.01, 0.1, 0.2, 0.3), gamma =
 c(0, 0.1, 0.5), colsample_bytree = 1, min_child_weight = 1, subsample = 1.

 The returned XGBoost classifier used all data in argument df without partition.
 When hyper_choice = "defaults", this returned classifier will use the default
 hyperparameters. When hyper_choice = "tune", this returned classifier will use
 best tuned hyperparameters.
}
\examples{
final_model <- train_model(df_time, vec_label = whitestork_acc_sorted[,ncol(whitestork_acc_sorted)])
}
